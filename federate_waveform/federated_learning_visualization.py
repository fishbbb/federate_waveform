# -*- coding: utf-8 -*-
"""
è”é‚¦å­¦ä¹ å¯è§†åŒ–ç›‘æ§æ¨¡å—
ç”¨äºç›‘æ§å’Œå±•ç¤ºè”é‚¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹
"""

import os
import json
import time
import threading
import subprocess
import signal
import sys
from datetime import datetime
from typing import Dict, List, Optional, Any
from collections import defaultdict
from pathlib import Path

# å¯¼å…¥torchç”¨äºè¯»å–.pthæ–‡ä»¶
try:
    import torch
except ImportError:
    torch = None
    print("è­¦å‘Š: torchæœªå®‰è£…ï¼Œæ— æ³•è¯»å–æ•°æ®æ–‡ä»¶ä¿¡æ¯")

# å¯¼å…¥torchç”¨äºè¯»å–.pthæ–‡ä»¶
try:
    import torch
except ImportError:
    torch = None
    print("è­¦å‘Š: torchæœªå®‰è£…ï¼Œæ— æ³•è¯»å–æ•°æ®æ–‡ä»¶ä¿¡æ¯")

# æ£€æŸ¥å¹¶å¯¼å…¥Flaskç›¸å…³ä¾èµ–
try:
    from flask import Flask, render_template, jsonify, request
except ImportError:
    print("=" * 60)
    print("é”™è¯¯: Flaskæœªå®‰è£…")
    print("=" * 60)
    print("è¯·è¿è¡Œ: pip install Flask==2.3.3")
    print("æˆ–è¿è¡Œ: pip install -r visualization_requirements.txt")
    print("=" * 60)
    raise

try:
    from flask_socketio import SocketIO, emit
except ImportError:
    print("=" * 60)
    print("é”™è¯¯: flask-socketioæœªå®‰è£…")
    print("=" * 60)
    print("è¯·è¿è¡Œ: pip install flask-socketio==5.3.5")
    print("æˆ–è¿è¡Œ: pip install -r visualization_requirements.txt")
    print("=" * 60)
    raise

# å°è¯•ä½¿ç”¨eventletï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨threading
try:
    import eventlet
    # ä½¿ç”¨eventletä½œä¸ºå¼‚æ­¥åç«¯
    eventlet.monkey_patch()
    async_mode = 'eventlet'
except ImportError:
    async_mode = 'threading'
    print("è­¦å‘Š: eventletæœªå®‰è£…ï¼Œä½¿ç”¨threadingæ¨¡å¼ã€‚")
    print("ä¸ºè·å¾—æ›´å¥½æ€§èƒ½ï¼Œå»ºè®®å®‰è£…: pip install eventlet==0.33.3")

# è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
# ä»federate_waveformæ–‡ä»¶å¤¹å‘ä¸Šåˆ°æ ¹ç›®å½•ï¼Œç„¶åè®¿é—®visualization
current_dir = os.path.dirname(os.path.abspath(__file__))
base_dir = os.path.dirname(current_dir)  # å‘ä¸Šåˆ°æ ¹ç›®å½•
template_dir = os.path.join(base_dir, 'visualization', 'templates')
static_dir = os.path.join(base_dir, 'visualization', 'static')

# éªŒè¯è·¯å¾„
if not os.path.exists(template_dir):
    print(f"è­¦å‘Š: æ¨¡æ¿ç›®å½•ä¸å­˜åœ¨: {template_dir}")
    print(f"å½“å‰ç›®å½•: {current_dir}")
    print(f"åŸºç¡€ç›®å½•: {base_dir}")
if not os.path.exists(static_dir):
    print(f"è­¦å‘Š: é™æ€æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨: {static_dir}")

# ç¡®ä¿ç›®å½•å­˜åœ¨
if not os.path.exists(template_dir):
    raise FileNotFoundError(f"Template directory not found: {template_dir}")
if not os.path.exists(static_dir):
    raise FileNotFoundError(f"Static directory not found: {static_dir}")

app = Flask(__name__, 
            template_folder=template_dir,
            static_folder=static_dir)
app.config['SECRET_KEY'] = 'federated-learning-visualization'
socketio = SocketIO(app, cors_allowed_origins="*", async_mode=async_mode)

# å…¨å±€çŠ¶æ€å­˜å‚¨
training_state = {
    'experiment_running': False,
    'current_round': 0,
    'total_rounds': 0,
    'nodes': {},  # {node_id: {status, data_size, metrics, ...}}
    'round_history': [],  # [{round, timestamp, nodes_metrics, global_metrics}]
    'global_metrics': {
        'loss': [],
        'f1': [],
        'accuracy': [],
        'rounds': []
    },
    'detailed_status': {
        'round_start_time': None,  # å½“å‰è½®æ¬¡å¼€å§‹æ—¶é—´
        'round_times': [],  # æ¯è½®è€—æ—¶ï¼ˆç§’ï¼‰
        'current_metrics': {
            'loss': None,
            'f1': None,
            'accuracy': None
        },
        'nodes_training': {}  # {node_id: {status, start_time, progress}}
    },
    'start_time': None,
    'end_time': None,
    'experiment_config': {},
    'active_nodes': set(),  # æ¿€æ´»çš„èŠ‚ç‚¹é›†åˆï¼ˆä¸å†ä½¿ç”¨å¤–éƒ¨è¿›ç¨‹ï¼‰
    'logs': [],  # æ—¥å¿—åˆ—è¡¨
    'analysis_data': {}  # åˆ†ææ•°æ®
}

# èŠ‚ç‚¹çŠ¶æ€æšä¸¾
NODE_STATUS = {
    'IDLE': 'idle',
    'TRAINING': 'training',
    'UPLOADING': 'uploading',
    'COMPLETED': 'completed',
    'ERROR': 'error'
}


class FederatedLearningMonitor:
    """è”é‚¦å­¦ä¹ ç›‘æ§å™¨"""
    
    def __init__(self):
        self.state = training_state
        self.callbacks = []
    
    def register_callback(self, callback):
        """æ³¨å†ŒçŠ¶æ€æ›´æ–°å›è°ƒ"""
        self.callbacks.append(callback)
    
    def notify_callbacks(self, event_type, data):
        """é€šçŸ¥æ‰€æœ‰å›è°ƒå‡½æ•°"""
        for callback in self.callbacks:
            try:
                callback(event_type, data)
            except Exception as e:
                print(f"Error in callback: {e}")
    
    def start_experiment(self, config: Dict):
        """å¼€å§‹å®éªŒ"""
        self.state['experiment_running'] = True
        self.state['current_round'] = 0
        self.state['total_rounds'] = config.get('round_limit', 5)
        self.state['start_time'] = datetime.now().isoformat()
        self.state['experiment_config'] = config
        self.state['nodes'] = {}
        self.state['round_history'] = []
        
        # åˆå§‹åŒ–å…¨å±€æŒ‡æ ‡æ•°ç»„ï¼ˆé¢„åˆ†é…ç©ºé—´ï¼‰
        total_rounds = config.get('round_limit', 5)
        self.state['global_metrics'] = {
            'loss': [0.0] * total_rounds,
            'f1': [0.0] * total_rounds,
            'accuracy': [0.0] * total_rounds,
            'rounds': list(range(total_rounds))
        }
        
        # åˆå§‹åŒ–è¯¦ç»†çŠ¶æ€
        self.state['detailed_status'] = {
            'round_start_time': None,
            'round_times': [],
            'current_metrics': {
                'loss': None,
                'f1': None,
                'accuracy': None
            },
            'nodes_training': {}
        }
        
        print(f"å®éªŒå·²å¯åŠ¨: {total_rounds} è½®è®­ç»ƒ")
        print(f"æŒ‡æ ‡æ•°ç»„å·²åˆå§‹åŒ–: loss={len(self.state['global_metrics']['loss'])}, f1={len(self.state['global_metrics']['f1'])}, accuracy={len(self.state['global_metrics']['accuracy'])}")
        
        self.notify_callbacks('experiment_started', {
            'config': config,
            'timestamp': self.state['start_time']
        })
    
    def end_experiment(self):
        """ç»“æŸå®éªŒ"""
        self.state['experiment_running'] = False
        self.state['end_time'] = datetime.now().isoformat()
        
        self.notify_callbacks('experiment_ended', {
            'timestamp': self.state['end_time']
        })
    
    def update_node_status(self, node_id: str, status: str, **kwargs):
        """æ›´æ–°èŠ‚ç‚¹çŠ¶æ€"""
        # éªŒè¯èŠ‚ç‚¹IDæ˜¯å¦æœ‰æ•ˆ
        if node_id not in ['node_1', 'node_2', 'node_3']:
            # å°è¯•æ ‡å‡†åŒ–èŠ‚ç‚¹ID
            if node_id.startswith('node_'):
                # å·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œä½†ä¸åœ¨æœ‰æ•ˆåˆ—è¡¨ä¸­ï¼Œå¯èƒ½æ˜¯æ— æ•ˆID
                return
            # å°è¯•æå–æ•°å­—
            import re
            num_match = re.search(r'(\d+)', node_id)
            if num_match and int(num_match.group(1)) in [1, 2, 3]:
                node_id = f'node_{num_match.group(1)}'
            else:
                # æ— æ•ˆçš„èŠ‚ç‚¹IDï¼Œå¿½ç•¥
                return
        
        if node_id not in self.state['nodes']:
            self.state['nodes'][node_id] = {
                'id': node_id,
                'status': status,
                'data_size': 0,
                'metrics': {},
                'last_update': datetime.now().isoformat()
            }
        
        self.state['nodes'][node_id]['status'] = status
        self.state['nodes'][node_id]['last_update'] = datetime.now().isoformat()
        
        for key, value in kwargs.items():
            self.state['nodes'][node_id][key] = value
        
        self.notify_callbacks('node_status_updated', {
            'node_id': node_id,
            'status': status,
            'data': self.state['nodes'][node_id]
        })
    
    def start_round(self, round_num: int):
        """å¼€å§‹æ–°çš„ä¸€è½®è®­ç»ƒ"""
        # è®°å½•ä¸Šä¸€è½®çš„æ—¶é—´ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'detailed_status' in self.state and self.state['detailed_status'].get('round_start_time'):
            prev_round_time = (datetime.now() - datetime.fromisoformat(
                self.state['detailed_status']['round_start_time']
            )).total_seconds()
            if 'round_times' not in self.state['detailed_status']:
                self.state['detailed_status']['round_times'] = []
            self.state['detailed_status']['round_times'].append(prev_round_time)
        
        # è®°å½•å½“å‰è½®æ¬¡å¼€å§‹æ—¶é—´
        if 'detailed_status' not in self.state:
            self.state['detailed_status'] = {
                'round_start_time': None,
                'round_times': [],
                'current_metrics': {'loss': None, 'f1': None, 'accuracy': None},
                'nodes_training': {}
            }
        self.state['detailed_status']['round_start_time'] = datetime.now().isoformat()
        
        self.state['current_round'] = round_num
        
        # ç¡®ä¿è½®æ¬¡æŒ‡æ ‡æ•°ç»„è¶³å¤Ÿå¤§
        while len(self.state['global_metrics']['rounds']) <= round_num:
            self.state['global_metrics']['rounds'].append(len(self.state['global_metrics']['rounds']))
            self.state['global_metrics']['loss'].append(0.0)
            self.state['global_metrics']['f1'].append(0.0)
            self.state['global_metrics']['accuracy'].append(0.0)
        
        # é‡ç½®æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€ä¸º idleï¼ˆè®­ç»ƒå¼€å§‹åä¼šæ›´æ–°ä¸º trainingï¼‰
        for node_id in self.state['nodes']:
            self.state['nodes'][node_id]['status'] = NODE_STATUS['IDLE']
        
        self.notify_callbacks('round_started', {
            'round': round_num,
            'timestamp': datetime.now().isoformat()
        })
    
    def update_round_metrics(self, round_num: int, node_metrics: Dict, global_metrics: Optional[Dict] = None):
        """æ›´æ–°è½®æ¬¡æŒ‡æ ‡"""
        round_data = {
            'round': round_num,
            'timestamp': datetime.now().isoformat(),
            'nodes': node_metrics,
            'global': global_metrics or {}
        }
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        while len(self.state['round_history']) <= round_num:
            self.state['round_history'].append({
                'round': len(self.state['round_history']),
                'timestamp': datetime.now().isoformat(),
                'nodes': {},
                'global': {}
            })
        self.state['round_history'][round_num] = round_data
        
        # æ›´æ–°å…¨å±€æŒ‡æ ‡ï¼ˆç¡®ä¿æ•°ç»„é•¿åº¦ä¸€è‡´ï¼‰
        while len(self.state['global_metrics']['rounds']) <= round_num:
            self.state['global_metrics']['rounds'].append(len(self.state['global_metrics']['rounds']))
            self.state['global_metrics']['loss'].append(0.0)
            self.state['global_metrics']['f1'].append(0.0)
            self.state['global_metrics']['accuracy'].append(0.0)
        
        # æ›´æ–°æŒ‡å®šè½®æ¬¡çš„æŒ‡æ ‡
        if global_metrics:
            if 'loss' in global_metrics:
                self.state['global_metrics']['loss'][round_num] = global_metrics['loss']
            if 'f1' in global_metrics:
                self.state['global_metrics']['f1'][round_num] = global_metrics['f1']
            if 'accuracy' in global_metrics:
                self.state['global_metrics']['accuracy'][round_num] = global_metrics['accuracy']
        
        # æ›´æ–°è¯¦ç»†çŠ¶æ€ä¸­çš„å½“å‰è½®æ¬¡æŒ‡æ ‡ï¼ˆè¿™æ˜¯å‰ç«¯æ˜¾ç¤ºçš„åœ°æ–¹ï¼‰
        if 'detailed_status' not in self.state:
            self.state['detailed_status'] = {
                'round_start_time': None,
                'round_times': [],
                'current_metrics': {'loss': None, 'f1': None, 'accuracy': None},
                'nodes_training': {}
            }
        
        if global_metrics:
            self.state['detailed_status']['current_metrics'] = {
                'loss': global_metrics.get('loss'),
                'f1': global_metrics.get('f1'),
                'accuracy': global_metrics.get('accuracy')
            }
        
        # æ›´æ–°èŠ‚ç‚¹è®­ç»ƒçŠ¶æ€
        if 'nodes_training' not in self.state['detailed_status']:
            self.state['detailed_status']['nodes_training'] = {}
        
        for node_id, metrics in node_metrics.items():
            self.state['detailed_status']['nodes_training'][node_id] = {
                'status': 'completed',
                'loss': metrics.get('loss'),
                'f1': metrics.get('f1'),
                'accuracy': metrics.get('accuracy'),
                'samples': metrics.get('samples', 0)
            }
        
        self.notify_callbacks('round_metrics_updated', round_data)
    
    def get_state(self) -> Dict:
        """è·å–å½“å‰çŠ¶æ€ï¼ˆæ’é™¤ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡ï¼‰"""
        state = self.state.copy()
        
        # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        # training_process å’Œ node_processes åŒ…å« Popen å¯¹è±¡ï¼Œä¸èƒ½åºåˆ—åŒ–
        # åªè¿”å›è¿›ç¨‹çš„ PID ä¿¡æ¯
        # ç¡®ä¿ current_round å’Œ total_rounds æœ‰é»˜è®¤å€¼
        current_round = state.get('current_round', 0)
        total_rounds = state.get('total_rounds', 0)
        
        # å¦‚æœ total_rounds ä¸º 0 ä½† experiment_config å­˜åœ¨ï¼Œå°è¯•ä»é…ç½®ä¸­è·å–
        if total_rounds == 0 and state.get('experiment_config'):
            total_rounds = state.get('experiment_config', {}).get('round_limit', 0)
        
        serializable_state = {
            'experiment_running': state.get('experiment_running', False),
            'current_round': current_round,
            'total_rounds': total_rounds,
            'start_time': state.get('start_time'),
            'end_time': state.get('end_time'),
            'experiment_config': state.get('experiment_config', {}),
            'nodes': state.get('nodes', {}),
            'round_history': state.get('round_history', []),
            'global_metrics': state.get('global_metrics', {
                'loss': [],
                'f1': [],
                'accuracy': [],
                'rounds': []
            }),
            'detailed_status': state.get('detailed_status', {
                'round_start_time': None,
                'round_times': [],
                'current_metrics': {
                    'loss': None,
                    'f1': None,
                    'accuracy': None
                },
                'nodes_training': {}
            })
        }
        
        # æ·»åŠ è¿›ç¨‹ä¿¡æ¯ï¼ˆä»… PIDï¼‰
        if 'training_process' in state and state['training_process']:
            process = state['training_process']
            serializable_state['training_process_pid'] = process.pid if hasattr(process, 'pid') else None
        else:
            serializable_state['training_process_pid'] = None
        
        # æ·»åŠ èŠ‚ç‚¹è¿›ç¨‹ä¿¡æ¯ï¼ˆä»… PIDï¼‰
        node_processes_info = {}
        if 'node_processes' in state:
            for node_id, process in state['node_processes'].items():
                if process and hasattr(process, 'pid'):
                    node_processes_info[node_id] = {
                        'pid': process.pid,
                        'running': process.poll() is None
                    }
        serializable_state['node_processes'] = node_processes_info
        
        return serializable_state


# åˆ›å»ºå…¨å±€ç›‘æ§å™¨å®ä¾‹
monitor = FederatedLearningMonitor()


# WebSocketäº‹ä»¶å¤„ç†
@socketio.on('connect')
def handle_connect():
    """å®¢æˆ·ç«¯è¿æ¥"""
    print('Client connected')
    emit('connected', {'message': 'Connected to federated learning monitor'})
    # å‘é€å½“å‰çŠ¶æ€
    emit('state_update', monitor.get_state())


@socketio.on('disconnect')
def handle_disconnect():
    """å®¢æˆ·ç«¯æ–­å¼€è¿æ¥"""
    print('Client disconnected')


@socketio.on('request_state')
def handle_request_state():
    """å®¢æˆ·ç«¯è¯·æ±‚å½“å‰çŠ¶æ€"""
    emit('state_update', monitor.get_state())


# ç›‘æ§å™¨å›è°ƒå‡½æ•° - é€šè¿‡WebSocketæ¨é€æ›´æ–°
def broadcast_update(event_type, data):
    """å¹¿æ’­æ›´æ–°åˆ°æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯"""
    socketio.emit('update', {
        'event_type': event_type,
        'data': data,
        'timestamp': datetime.now().isoformat()
    })


# æ³¨å†Œå›è°ƒ
monitor.register_callback(broadcast_update)


# HTTPè·¯ç”±
@app.route('/')
def index():
    """ä¸»é¡µé¢"""
    try:
        # è°ƒè¯•ä¿¡æ¯
        print(f"Template directory: {template_dir}")
        print(f"Template exists: {os.path.exists(template_dir)}")
        index_path = os.path.join(template_dir, 'index.html')
        print(f"Index.html path: {index_path}")
        print(f"Index.html exists: {os.path.exists(index_path)}")
        if os.path.exists(index_path):
            with open(index_path, 'r', encoding='utf-8') as f:
                content = f.read()
                print(f"Template file size: {len(content)} bytes")
        return render_template('index.html')
    except Exception as e:
        print(f"Error rendering template: {e}")
        import traceback
        traceback.print_exc()
        return f"<h1>Error</h1><p>{str(e)}</p><pre>{traceback.format_exc()}</pre>", 500


@app.route('/test')
def test():
    """æµ‹è¯•è·¯ç”±"""
    return jsonify({
        'status': 'ok',
        'template_dir': template_dir,
        'static_dir': static_dir,
        'template_exists': os.path.exists(template_dir),
        'static_exists': os.path.exists(static_dir)
    })


@app.route('/simple')
def simple():
    """ç®€å•æµ‹è¯•é¡µé¢"""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Test Page</title>
    </head>
    <body>
        <h1>Server is working!</h1>
        <p>If you see this, the Flask server is running correctly.</p>
        <p><a href="/">Go to main page</a></p>
    </body>
    </html>
    """


@app.route('/api/state')
def get_state():
    """è·å–å½“å‰çŠ¶æ€API"""
    return jsonify(monitor.get_state())


@app.route('/api/nodes')
def get_nodes():
    """è·å–èŠ‚ç‚¹ä¿¡æ¯API"""
    return jsonify({
        'nodes': list(monitor.state['nodes'].values()),
        'count': len(monitor.state['nodes'])
    })


@app.route('/api/metrics')
def get_metrics():
    """è·å–æŒ‡æ ‡æ•°æ®API"""
    return jsonify({
        'global_metrics': monitor.state['global_metrics'],
        'round_history': monitor.state['round_history']
    })


@app.route('/api/rounds/<int:round_num>')
def get_round(round_num):
    """è·å–ç‰¹å®šè½®æ¬¡çš„æ•°æ®"""
    if round_num < len(monitor.state['round_history']):
        return jsonify(monitor.state['round_history'][round_num])
    return jsonify({'error': 'Round not found'}), 404


# ==================== æ§åˆ¶åŠŸèƒ½API ====================

# @app.route('/api/nodes/start', methods=['POST'])
# def start_node():
#     """å¯åŠ¨èŠ‚ç‚¹"""
#     data = request.get_json()
#     node_id = data.get('node_id', 'node_1')
#     node_path = data.get('node_path', f'fbm-node-{node_id.split("_")[-1]}')
    
#     try:
#         base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
#         node_data_dir = os.path.join(base_dir, 'federated_data', node_id)
#         node_full_path = os.path.join(base_dir, node_path)
        
#         # æ£€æŸ¥å¹¶ç¡®å®š fedbiomed å‘½ä»¤è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨ fb_env ä¸­çš„ï¼‰
#         fb_env_path = os.path.join(base_dir, 'fb_env', 'bin', 'fedbiomed')
        
#         # ä¼˜å…ˆä½¿ç”¨ fb_env ä¸­çš„ fedbiomed
#         if os.path.exists(fb_env_path):
#             fedbiomed_cmd = fb_env_path
#         else:
#             # å›é€€åˆ°ç³»ç»Ÿè·¯å¾„
#             fedbiomed_cmd = 'fedbiomed'
        
#         # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å·²ç»åœ¨è¿è¡Œ
#         if node_id in training_state['node_processes']:
#             process = training_state['node_processes'][node_id]
#             if process.poll() is None:  # è¿›ç¨‹è¿˜åœ¨è¿è¡Œ
#                 return jsonify({
#                     'success': False,
#                     'error': f'èŠ‚ç‚¹ {node_id} å·²ç»åœ¨è¿è¡Œä¸­ (PID: {process.pid})'
#                 }), 400
        
#         # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
#         if not os.path.exists(node_data_dir):
#             return jsonify({
#                 'success': False,
#                 'error': f'èŠ‚ç‚¹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {node_data_dir}ã€‚è¯·å…ˆè¿è¡Œ prepare_federated_data.py å‡†å¤‡æ•°æ®ã€‚'
#             }), 404
        
#         # æ£€æŸ¥ fedbiomed å‘½ä»¤æ˜¯å¦å¯ç”¨ï¼ˆä½¿ç”¨å·²ç¡®å®šçš„ fedbiomed_cmdï¼‰
#         try:
#             subprocess.run([fedbiomed_cmd, '--version'], 
#                          capture_output=True, check=True, timeout=5)
#             add_log(f'âœ… fedbiomed å‘½ä»¤å¯ç”¨: {fedbiomed_cmd}', level='info')
#         except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
#             return jsonify({
#                 'success': False,
#                 'error': f'fedbiomed å‘½ä»¤ä¸å¯ç”¨ ({fedbiomed_cmd})ã€‚è¯·ç¡®ä¿å·²æ¿€æ´» fb_env ç¯å¢ƒå¹¶å®‰è£…äº† Fed-BioMedã€‚'
#             }), 500
        
#         # ç¡®ä¿èŠ‚ç‚¹ç›®å½•å­˜åœ¨
#         os.makedirs(node_full_path, exist_ok=True)
        
#         # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²é…ç½®
#         dataset_configured = False
#         dataset_count = 0
#         try:
#             import glob
#             dataset_db_path = os.path.join(node_full_path, 'var', 'db_*.json')
#             existing_datasets = glob.glob(dataset_db_path)
#             dataset_count = len(existing_datasets)
#             dataset_configured = dataset_count > 0
#         except Exception as e:
#             add_log(f'æ£€æŸ¥æ•°æ®é›†é…ç½®æ—¶å‡ºé”™: {e}', level='warning')
        
#         if not dataset_configured:
#             add_log(f'âš ï¸ è­¦å‘Š: èŠ‚ç‚¹ {node_id} å°šæœªé…ç½®æ•°æ®é›†ï¼', level='error')
#             add_log(f'   èŠ‚ç‚¹å¯ä»¥å¯åŠ¨ï¼Œä½†æ— æ³•å‚ä¸è®­ç»ƒã€‚', level='warning')
#             add_log(f'   é…ç½®æ–¹æ³•: åœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼ˆç¡®ä¿åœ¨ fb_env ç¯å¢ƒä¸­ï¼‰:', level='info')
#             add_log(f'   source fb_env/bin/activate', level='info')
#             add_log(f'   fedbiomed node --path {node_path} dataset add', level='info')
#             add_log(f'   ç„¶åé€‰æ‹©æ•°æ®æ–‡ä»¶: {node_data_dir}/train.pth', level='info')
#             add_log(f'   æ ‡ç­¾ä½¿ç”¨: #hypotension #waveform #ecg #uci2', level='info')
#         else:
#             add_log(f'âœ… èŠ‚ç‚¹ {node_id} å·²é…ç½® {dataset_count} ä¸ªæ•°æ®é›†', level='info')
        
#         # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆç”¨äºè®¾å¤‡ä»¿çœŸï¼‰
#         env = os.environ.copy()
#         env['FB_NODE_PATH'] = node_path
        
#         # å¯åŠ¨èŠ‚ç‚¹è¿›ç¨‹
#         # æ³¨æ„ï¼šfedbiomed node start ä¼šæŒç»­è¿è¡Œï¼Œéœ€è¦ä½œä¸ºåå°è¿›ç¨‹
#         # ä½¿ç”¨ç¡®å®šçš„ fedbiomed å‘½ä»¤è·¯å¾„ï¼ˆä¼˜å…ˆ fb_envï¼‰
#         if sys.platform == 'win32':
#             # Windowså¹³å°
#             cmd = [fedbiomed_cmd, 'node', '--path', node_full_path, 'start']
#             process = subprocess.Popen(
#                 cmd,
#                 cwd=node_data_dir,
#                 env=env,
#                 stdout=subprocess.PIPE,
#                 stderr=subprocess.PIPE,
#                 text=True,
#                 creationflags=subprocess.CREATE_NEW_PROCESS_GROUP
#             )
#         else:
#             # Unixå¹³å°ï¼ˆmacOS/Linuxï¼‰- ä½¿ç”¨shellä»¥ä¾¿æ­£ç¡®å¤„ç†è·¯å¾„å’Œç¯å¢ƒå˜é‡
#             # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ fedbiomed
#             cmd = f'cd "{node_data_dir}" && "{fedbiomed_cmd}" node --path "{node_full_path}" start'
#             process = subprocess.Popen(
#                 cmd,
#                 shell=True,
#                 cwd=node_data_dir,
#                 env=env,
#                 stdout=subprocess.PIPE,
#                 stderr=subprocess.PIPE,
#                 text=True,
#                 preexec_fn=os.setsid if hasattr(os, 'setsid') else None
#             )
        
#         # ç­‰å¾…ä¸€å°æ®µæ—¶é—´æ£€æŸ¥è¿›ç¨‹æ˜¯å¦æˆåŠŸå¯åŠ¨
#         time.sleep(2)
#         if process.poll() is not None:
#             # è¿›ç¨‹å·²ç»é€€å‡ºï¼Œè¯»å–é”™è¯¯ä¿¡æ¯
#             try:
#                 stdout, stderr = process.communicate(timeout=1)
#             except:
#                 stdout, stderr = '', ''
#             error_msg = (stderr.strip() or stdout.strip() or 'è¿›ç¨‹å¯åŠ¨åç«‹å³é€€å‡º')
            
#             # æä¾›æ›´å‹å¥½çš„é”™è¯¯æç¤º
#             if 'fedbiomed' in error_msg.lower() or 'command not found' in error_msg.lower():
#                 error_msg = 'fedbiomed å‘½ä»¤ä¸å¯ç”¨ã€‚è¯·ç¡®ä¿å·²æ¿€æ´» fb_env ç¯å¢ƒã€‚'
#             elif 'dataset' in error_msg.lower() or 'not found' in error_msg.lower():
#                 error_msg = 'æ•°æ®é›†æœªé…ç½®ã€‚è¯·å…ˆè¿è¡Œ: fedbiomed node --path ' + node_path + ' dataset add'
            
#             return jsonify({
#                 'success': False,
#                 'error': f'èŠ‚ç‚¹å¯åŠ¨å¤±è´¥: {error_msg}'
#             }), 500
        
#         training_state['node_processes'][node_id] = process
        
#         # è¯»å–èŠ‚ç‚¹å®é™…æ•°æ®é‡
#         data_size = 0
#         try:
#             train_pth_path = os.path.join(node_data_dir, 'train.pth')
#             if os.path.exists(train_pth_path):
#                 train_data = torch.load(train_pth_path, map_location='cpu', weights_only=False)
#                 if isinstance(train_data, dict) and 'train' in train_data:
#                     data_size = len(train_data['train'])
#                     add_log(f'èŠ‚ç‚¹ {node_id} æ•°æ®é‡: {data_size} ä¸ªæ ·æœ¬', level='info')
#                 else:
#                     add_log(f'è­¦å‘Š: æ— æ³•è§£æèŠ‚ç‚¹ {node_id} çš„æ•°æ®æ–‡ä»¶æ ¼å¼', level='warning')
#             else:
#                 add_log(f'è­¦å‘Š: èŠ‚ç‚¹ {node_id} çš„æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_pth_path}', level='warning')
#         except Exception as e:
#             add_log(f'è¯»å–èŠ‚ç‚¹ {node_id} æ•°æ®é‡æ—¶å‡ºé”™: {str(e)}', level='warning')
#             # å°è¯•ä»data_info.txtè¯»å–
#             try:
#                 data_info_path = os.path.join(node_data_dir, 'data_info.txt')
#                 if os.path.exists(data_info_path):
#                     with open(data_info_path, 'r') as f:
#                         for line in f:
#                             if 'train' in line.lower() and ('samples' in line.lower() or 'æ ·æœ¬' in line):
#                                 import re
#                                 numbers = re.findall(r'\d+', line)
#                                 if numbers:
#                                     data_size = int(numbers[0])
#                                     break
#             except:
#                 pass
        
#         # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€ï¼ˆä½¿ç”¨å®é™…æ•°æ®é‡ï¼‰
#         # èŠ‚ç‚¹åˆšå¯åŠ¨æ—¶çŠ¶æ€ä¸º 'running'ï¼Œç­‰å¾…è¿æ¥åˆ° researcher
#         monitor.update_node_status(node_id, 'running', data_size=data_size)
        
#         add_log(f'âœ… èŠ‚ç‚¹ {node_id} å·²å¯åŠ¨ (PID: {process.pid}, Path: {node_path}, æ•°æ®é‡: {data_size})', level='info')
#         add_log(f'   èŠ‚ç‚¹æ­£åœ¨å°è¯•è¿æ¥åˆ° researcher...', level='info')
        
#         return jsonify({
#             'success': True,
#             'node_id': node_id,
#             'pid': process.pid,
#             'node_path': node_path,
#             'message': f'èŠ‚ç‚¹ {node_id} å¯åŠ¨æˆåŠŸ'
#         })
#     except Exception as e:
#         import traceback
#         error_detail = traceback.format_exc()
#         add_log(f'å¯åŠ¨èŠ‚ç‚¹ {node_id} å¤±è´¥: {str(e)}\n{error_detail}', level='error')
#         return jsonify({
#             'success': False,
#             'error': str(e)
#         }), 500
@app.route('/api/nodes/start', methods=['POST'])
def start_node():
    """å¯åŠ¨èŠ‚ç‚¹ï¼ˆé€»è¾‘æŠ½è±¡ï¼Œä¸å†å¯åŠ¨å¤–éƒ¨è¿›ç¨‹ï¼‰"""
    data = request.get_json()
    node_id = data.get('node_id', 'node_1')
    
    try:
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        node_data_dir = os.path.join(base_dir, 'federated_data', node_id)
        
        # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å·²ç»æ¿€æ´»
        if node_id in training_state.get('active_nodes', set()):
            return jsonify({
                'success': False,
                'error': f'èŠ‚ç‚¹ {node_id} å·²ç»æ¿€æ´»'
            }), 400
        
        # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(node_data_dir):
            return jsonify({
                'success': False,
                'error': f'èŠ‚ç‚¹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {node_data_dir}ã€‚è¯·å…ˆè¿è¡Œ prepare_federated_data.py å‡†å¤‡æ•°æ®ã€‚'
            }), 404
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        train_pth_path = os.path.join(node_data_dir, 'train.pth')
        if not os.path.exists(train_pth_path):
            return jsonify({
                'success': False,
                'error': f'èŠ‚ç‚¹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_pth_path}'
            }), 404
        
        # è¯»å–èŠ‚ç‚¹å®é™…æ•°æ®é‡
        data_size = 0
        try:
            train_data = torch.load(train_pth_path, map_location='cpu', weights_only=False)
            if isinstance(train_data, dict) and 'train' in train_data:
                data_size = len(train_data['train'])
                add_log(f'èŠ‚ç‚¹ {node_id} æ•°æ®é‡: {data_size} ä¸ªæ ·æœ¬', level='info')
            else:
                add_log(f'è­¦å‘Š: æ— æ³•è§£æèŠ‚ç‚¹ {node_id} çš„æ•°æ®æ–‡ä»¶æ ¼å¼', level='warning')
        except Exception as e:
            add_log(f'è¯»å–èŠ‚ç‚¹ {node_id} æ•°æ®é‡æ—¶å‡ºé”™: {str(e)}', level='warning')
            # å°è¯•ä» data_info.txt è¯»å–
            try:
                data_info_path = os.path.join(node_data_dir, 'data_info.txt')
                if os.path.exists(data_info_path):
                    with open(data_info_path, 'r') as f:
                        for line in f:
                            if 'train' in line.lower() and ('samples' in line.lower() or 'æ ·æœ¬' in line):
                                import re
                                numbers = re.findall(r'\d+', line)
                                if numbers:
                                    data_size = int(numbers[0])
                                    break
            except Exception:
                pass
        
        # æ ‡è®°èŠ‚ç‚¹ä¸ºæ¿€æ´»çŠ¶æ€
        if 'active_nodes' not in training_state:
            training_state['active_nodes'] = set()
        training_state['active_nodes'].add(node_id)
        
        # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
        monitor.update_node_status(node_id, 'running', data_size=data_size)
        
        add_log(f'âœ… èŠ‚ç‚¹ {node_id} å·²æ¿€æ´» (æ•°æ®é‡: {data_size})', level='info')
        add_log(f'   èŠ‚ç‚¹å·²å‡†å¤‡å¥½å‚ä¸è”é‚¦è®­ç»ƒ', level='info')
        
        return jsonify({
            'success': True,
            'node_id': node_id,
            'data_size': data_size,
            'message': f'èŠ‚ç‚¹ {node_id} æ¿€æ´»æˆåŠŸ'
        })
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        add_log(f'æ¿€æ´»èŠ‚ç‚¹ {node_id} å¤±è´¥: {str(e)}\n{error_detail}', level='error')
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/nodes/stop', methods=['POST'])
def stop_node():
    """åœæ­¢èŠ‚ç‚¹ï¼ˆå–æ¶ˆæ¿€æ´»ï¼‰"""
    data = request.get_json()
    node_id = data.get('node_id')
    
    if not node_id:
        return jsonify({'success': False, 'error': 'node_id is required'}), 400
    
    try:
        if 'active_nodes' in training_state and node_id in training_state['active_nodes']:
            training_state['active_nodes'].remove(node_id)
            monitor.update_node_status(node_id, 'idle')
            add_log(f'èŠ‚ç‚¹ {node_id} å·²å–æ¶ˆæ¿€æ´»', level='info')
            
            return jsonify({
                'success': True,
                'message': f'èŠ‚ç‚¹ {node_id} å·²å–æ¶ˆæ¿€æ´»'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'èŠ‚ç‚¹ {node_id} æœªæ‰¾åˆ°æˆ–æœªæ¿€æ´»'
            }), 404
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        add_log(f'å–æ¶ˆæ¿€æ´»èŠ‚ç‚¹ {node_id} å¤±è´¥: {str(e)}\n{error_detail}', level='error')
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/nodes/list', methods=['GET'])
def list_nodes():
    """åˆ—å‡ºæ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€ï¼ˆåŒ…æ‹¬å·²é…ç½®ä½†æœªå¯åŠ¨çš„èŠ‚ç‚¹ï¼‰"""
    nodes_info = []
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # ä»devices.yamlè¯»å–æ‰€æœ‰é…ç½®çš„èŠ‚ç‚¹
    try:
        devices_yaml = os.path.join(base_dir, 'federate_waveform', 'devices.yaml')
        if os.path.exists(devices_yaml):
            import yaml
            with open(devices_yaml, 'r') as f:
                devices_config = yaml.safe_load(f)
                configured_nodes = {d['id']: d for d in devices_config.get('devices', [])}
        else:
            configured_nodes = {}
    except:
        configured_nodes = {}
    
    # è·å–æ‰€æœ‰å¯èƒ½çš„èŠ‚ç‚¹IDï¼ˆä»é…ç½®æˆ–é»˜è®¤ï¼‰
    active_nodes = training_state.get('active_nodes', set())
    all_node_ids = set(configured_nodes.keys()) | active_nodes
    if not all_node_ids:
        # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨é»˜è®¤çš„3ä¸ªèŠ‚ç‚¹
        all_node_ids = {'node_1', 'node_2', 'node_3'}
    
    for node_id in sorted(all_node_ids):
        # æ£€æŸ¥æ¿€æ´»çŠ¶æ€
        if node_id in active_nodes:
            status = 'running'
            pid = None  # ä¸å†ä½¿ç”¨è¿›ç¨‹ID
        else:
            status = 'idle'
            pid = None
        
        # è·å–èŠ‚ç‚¹ä¿¡æ¯
        node_info = monitor.state['nodes'].get(node_id, {})
        device_config = configured_nodes.get(node_id, {})
        
        # å¦‚æœæ•°æ®é‡ä¸º0ï¼Œå°è¯•ä»æ–‡ä»¶è¯»å–
        data_size = node_info.get('data_size', 0)
        if data_size == 0:
            node_data_dir = os.path.join(base_dir, 'federated_data', node_id)
            train_pth_path = os.path.join(node_data_dir, 'train.pth')
            if os.path.exists(train_pth_path) and torch is not None:
                try:
                    train_data = torch.load(train_pth_path, map_location='cpu', weights_only=False)
                    if isinstance(train_data, dict) and 'train' in train_data:
                        data_size = len(train_data['train'])
                        # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
                        monitor.update_node_status(node_id, node_info.get('status', 'idle'), data_size=data_size)
                except Exception as e:
                    # å¦‚æœè¯»å–å¤±è´¥ï¼Œå°è¯•ä»data_info.txtè¯»å–
                    try:
                        data_info_path = os.path.join(node_data_dir, 'data_info.txt')
                        if os.path.exists(data_info_path):
                            with open(data_info_path, 'r') as f:
                                import re
                                for line in f:
                                    if 'train' in line.lower() and ('samples' in line.lower() or 'æ ·æœ¬' in line):
                                        numbers = re.findall(r'\d+', line)
                                        if numbers:
                                            data_size = int(numbers[0])
                                            monitor.update_node_status(node_id, node_info.get('status', 'idle'), data_size=data_size)
                                            break
                    except:
                        pass
        
        nodes_info.append({
            'node_id': node_id,
            'status': status,
            'pid': pid,
            'metrics': node_info.get('metrics', {}),
            'data_size': data_size,
            'device_type': device_config.get('type', 'unknown'),
            'compute_power': device_config.get('compute_power', 'unknown'),
            'online_pattern': device_config.get('online_pattern', 'always_on')
        })
    
    # è¿‡æ»¤æ‰æ— æ•ˆçš„èŠ‚ç‚¹IDï¼ˆå¦‚ node_sï¼‰
    valid_nodes = [n for n in nodes_info if n['node_id'] in ['node_1', 'node_2', 'node_3']]
    
    return jsonify({'nodes': valid_nodes})


@app.route('/api/training/start', methods=['POST'])
def start_training():
    """å¯åŠ¨è®­ç»ƒ"""
    data = request.get_json() or {}
    
    try:
        if training_state['experiment_running']:
            return jsonify({'error': 'Training already running'}), 400
        
        # è·å–è®­ç»ƒå‚æ•°
        rounds = data.get('rounds', 5)
        batch_size = data.get('batch_size', 128)
        learning_rate = data.get('learning_rate', 4e-5)
        
        # åˆå§‹åŒ–å®éªŒé…ç½®
        # æ³¨æ„ï¼šmodel_args ä¸­çš„ data_path ä»…ä½œä¸ºé»˜è®¤å€¼ï¼Œå®é™…è®­ç»ƒæ—¶ä¼šä½¿ç”¨èŠ‚ç‚¹ä¸Šçš„æ•°æ®
        config = {
            'round_limit': rounds,
            'tags': ['#hypotension', '#waveform', '#ecg', '#uci2'],
            'training_args': {
                'loader_args': {'batch_size': batch_size},
                'optimizer_args': {'lr': learning_rate},
                'epochs': 1
            },
            'model_args': {
                'batch_size': batch_size,
                'data_path': 'federated_data/node_1/train.pth',  # ä»…ä½œä¸ºé»˜è®¤å€¼ï¼Œå®é™…ä½¿ç”¨èŠ‚ç‚¹æ•°æ®
                'use_uci2': True,
                'uci2_base_dir': 'uci2_dataset'
            }
        }
        
        # å¼€å§‹ç›‘æ§å®éªŒ
        monitor.start_experiment(config)
        training_state['experiment_running'] = True
        
        # ç¡®ä¿çŠ¶æ€æ­£ç¡®åˆå§‹åŒ–
        monitor.state['current_round'] = 0
        monitor.state['total_rounds'] = rounds
        monitor.state['experiment_config'] = config
        
        # æŒ‡æ ‡æ•°ç»„åœ¨start_experimentä¸­å·²ç»é¢„åˆ†é…ï¼Œè¿™é‡Œä¸éœ€è¦å†æ¬¡åˆå§‹åŒ–
        
        # å‘é€åˆå§‹çŠ¶æ€æ›´æ–°ï¼ˆç«‹å³å‘é€ï¼Œç¡®ä¿å‰ç«¯æ”¶åˆ°ï¼‰
        socketio.emit('update', {
            'event_type': 'experiment_started',
            'data': {
                'config': config,
                'rounds': rounds,
                'current_round': 0,
                'total_rounds': rounds
            },
            'timestamp': datetime.now().isoformat()
        })
        
        # ç«‹å³å‘é€å®Œæ•´çŠ¶æ€æ›´æ–°
        state = monitor.get_state()
        socketio.emit('state_update', state)
        
        add_log(f'å®éªŒå·²å¯åŠ¨: æ€»è½®æ•°={rounds}, æ‰¹æ¬¡å¤§å°={batch_size}, å­¦ä¹ ç‡={learning_rate}', level='info')
        
        # æ£€æŸ¥æ¿€æ´»çš„èŠ‚ç‚¹
        active_nodes = training_state.get('active_nodes', set())
        if not active_nodes:
            add_log('âš ï¸ è­¦å‘Š: æ²¡æœ‰æ¿€æ´»çš„èŠ‚ç‚¹ï¼è¯·å…ˆæ¿€æ´»èŠ‚ç‚¹ã€‚', level='error')
            return jsonify({
                'success': False,
                'error': 'No active nodes. Please start nodes first.'
            }), 400
        
        add_log(f'âœ… {len(active_nodes)} ä¸ªèŠ‚ç‚¹å·²æ¿€æ´»: {", ".join(sorted(active_nodes))}', level='info')
        
        # å¯åŠ¨è®­ç»ƒï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼‰
        def run_training():
            try:
                # å¯¼å…¥è‡ªç ”çš„è”é‚¦è®­ç»ƒæ¨¡å—
                from federated_simulation_trainer import run_federated_training
                
                add_log('=' * 60, level='info')
                add_log('ğŸš€ å¼€å§‹è”é‚¦è®­ç»ƒ...', level='info')
                add_log('=' * 60, level='info')
                
                # ç›´æ¥è°ƒç”¨è®­ç»ƒå‡½æ•°
                # ä¼ é€’æ¿€æ´»çš„èŠ‚ç‚¹åˆ—è¡¨ï¼ˆè®­ç»ƒé¡ºåºå°†æŒ‰ online_pattern ä¼˜å…ˆçº§å†³å®šï¼‰
                active_nodes_set = training_state.get('active_nodes', set())
                active_nodes_list = list(active_nodes_set)
                
                # è·å–å‹ç¼©é…ç½®ï¼ˆä»è®­ç»ƒå‚æ•°ä¸­ï¼‰
                enable_compression = data.get('enable_compression', False)
                compression_config = data.get('compression_config', {})
                
                result = run_federated_training(
                    config=config,
                    monitor_instance=monitor,
                    socketio_instance=socketio,
                    progress_callback=None,
                    active_nodes=active_nodes_list,
                    save_results=True,
                    results_dir='results',
                    enable_compression=enable_compression,
                    compression_config=compression_config
                )
                
                add_log('=' * 60, level='info')
                add_log('âœ… è”é‚¦è®­ç»ƒå®Œæˆï¼', level='info')
                add_log('=' * 60, level='info')
                
                # è®­ç»ƒå®Œæˆ
                monitor.end_experiment()
                training_state['experiment_running'] = False
                
                socketio.emit('update', {
                    'event_type': 'experiment_ended',
                    'data': {'timestamp': datetime.now().isoformat()},
                    'timestamp': datetime.now().isoformat()
                })
                socketio.emit('state_update', monitor.get_state())
                
            except Exception as e:
                add_log(f'è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {str(e)}', level='error')
                monitor.end_experiment()
                training_state['experiment_running'] = False
                import traceback
                traceback.print_exc()
                socketio.emit('update', {
                    'event_type': 'experiment_error',
                    'data': {'error': str(e)},
                    'timestamp': datetime.now().isoformat()
                })
        
        # åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨è®­ç»ƒ
        training_thread = threading.Thread(target=run_training, daemon=True)
        training_thread.start()
        
        add_log(f'è®­ç»ƒå·²å¯åŠ¨ (è½®æ•°: {rounds}, æ‰¹æ¬¡å¤§å°: {batch_size}, å­¦ä¹ ç‡: {learning_rate})')
        
        return jsonify({
            'success': True,
            'message': 'Training started successfully',
            'config': config
        })
    except Exception as e:
        add_log(f'å¯åŠ¨è®­ç»ƒå¤±è´¥: {str(e)}', level='error')
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


@app.route('/api/training/status', methods=['POST'])
def update_training_status():
    """æ¥æ”¶è®­ç»ƒçŠ¶æ€æ›´æ–°ï¼ˆç”±è®­ç»ƒè„šæœ¬è°ƒç”¨ï¼‰"""
    data = request.get_json()
    action = data.get('action')
    
    try:
        if action == 'start':
            config = data.get('config', {})
            monitor.start_experiment(config)
            training_state['experiment_running'] = True
            socketio.emit('update', {
                'event_type': 'experiment_started',
                'data': {'config': config},
                'timestamp': datetime.now().isoformat()
            })
            
        elif action == 'round_started':
            round_num = data.get('round', 0)
            monitor.start_round(round_num)
            monitor.state['current_round'] = round_num
            socketio.emit('update', {
                'event_type': 'round_started',
                'data': {'round': round_num},
                'timestamp': datetime.now().isoformat()
            })
            
        elif action == 'round_metrics':
            round_num = data.get('round', 0)
            metrics = data.get('metrics', {})
            
            # ç¡®ä¿æ•°ç»„é•¿åº¦è¶³å¤Ÿ
            while len(monitor.state['global_metrics']['rounds']) <= round_num:
                monitor.state['global_metrics']['rounds'].append(len(monitor.state['global_metrics']['rounds']))
                monitor.state['global_metrics']['loss'].append(0.0)
                monitor.state['global_metrics']['f1'].append(0.0)
                monitor.state['global_metrics']['accuracy'].append(0.0)
            
            # æ›´æ–°å…¨å±€æŒ‡æ ‡
            if 'loss' in metrics:
                monitor.state['global_metrics']['loss'][round_num] = metrics['loss']
            if 'f1' in metrics:
                monitor.state['global_metrics']['f1'][round_num] = metrics['f1']
            if 'accuracy' in metrics:
                monitor.state['global_metrics']['accuracy'][round_num] = metrics['accuracy']
            
            # æ›´æ–°è½®æ¬¡æŒ‡æ ‡
            monitor.update_round_metrics(round_num, {}, metrics)
            
            socketio.emit('update', {
                'event_type': 'round_metrics_updated',
                'data': {'round': round_num, 'global_metrics': metrics},
                'timestamp': datetime.now().isoformat()
            })
            
        elif action == 'node_status':
            node_id = data.get('node_id')
            status = data.get('status')
            node_metrics = data.get('metrics')
            monitor.update_node_status(node_id, status, metrics=node_metrics)
            
        elif action == 'end':
            monitor.end_experiment()
            training_state['experiment_running'] = False
            socketio.emit('update', {
                'event_type': 'experiment_ended',
                'data': {'timestamp': datetime.now().isoformat()},
                'timestamp': datetime.now().isoformat()
            })
        
        socketio.emit('state_update', monitor.get_state())
        return jsonify({'success': True})
        
    except Exception as e:
        add_log(f'æ›´æ–°è®­ç»ƒçŠ¶æ€å¤±è´¥: {str(e)}', level='error')
        return jsonify({'error': str(e)}), 500


@app.route('/api/training/stop', methods=['POST'])
def stop_training():
    """åœæ­¢è®­ç»ƒ"""
    try:
        if training_state['experiment_running']:
            # æ³¨æ„ï¼šç”±äºè®­ç»ƒåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼Œæ— æ³•ç›´æ¥åœæ­¢
            # å¯ä»¥é€šè¿‡è®¾ç½®æ ‡å¿—ä½æ¥åœæ­¢ï¼ˆéœ€è¦åœ¨è®­ç»ƒå¾ªç¯ä¸­æ£€æŸ¥ï¼‰
            monitor.end_experiment()
            training_state['experiment_running'] = False
            add_log('è®­ç»ƒå·²åœæ­¢', level='info')
            
            socketio.emit('update', {
                'event_type': 'experiment_ended',
                'data': {'timestamp': datetime.now().isoformat()},
                'timestamp': datetime.now().isoformat()
            })
            socketio.emit('state_update', monitor.get_state())
            
            return jsonify({'success': True, 'message': 'Training stopped'})
        else:
            return jsonify({'error': 'No training running'}), 404
    except Exception as e:
        add_log(f'åœæ­¢è®­ç»ƒå¤±è´¥: {str(e)}', level='error')
        return jsonify({'error': str(e)}), 500


@app.route('/api/logs', methods=['GET'])
def get_logs():
    """è·å–æ—¥å¿—"""
    limit = request.args.get('limit', 100, type=int)
    level = request.args.get('level', None)
    
    logs = training_state['logs']
    if level:
        logs = [log for log in logs if log.get('level') == level]
    
    return jsonify({'logs': logs[-limit:], 'total': len(logs)})


@app.route('/api/logs/clear', methods=['POST'])
def clear_logs():
    """æ¸…ç©ºæ—¥å¿—"""
    training_state['logs'] = []
    return jsonify({'success': True, 'message': 'Logs cleared'})


@app.route('/api/analysis/data', methods=['GET'])
def get_analysis_data():
    """è·å–åˆ†ææ•°æ®"""
    state = monitor.get_state()
    
    analysis = {
        'convergence': {
            'losses': state['global_metrics']['loss'],
            'f1_scores': state['global_metrics']['f1'],
            'accuracies': state['global_metrics']['accuracy'],
            'rounds': state['global_metrics']['rounds']
        },
        'node_performance': {},
        'training_time': {},
        'data_distribution': {}
    }
    
    # èŠ‚ç‚¹æ€§èƒ½åˆ†æ
    for node_id, node_info in state['nodes'].items():
        metrics = node_info.get('metrics', {})
        analysis['node_performance'][node_id] = {
            'avg_loss': metrics.get('loss', 0),
            'avg_f1': metrics.get('f1', 0),
            'avg_accuracy': metrics.get('accuracy', 0),
            'data_size': node_info.get('data_size', 0)
        }
    
    # è®­ç»ƒæ—¶é—´åˆ†æ
    for round_data in state['round_history']:
        round_num = round_data.get('round', 0)
        timestamp = round_data.get('timestamp', '')
        analysis['training_time'][round_num] = timestamp
    
    return jsonify(analysis)


@app.route('/api/analysis/convergence', methods=['GET'])
def get_convergence_analysis():
    """è·å–æ”¶æ•›åˆ†æ"""
    state = monitor.get_state()
    losses = state['global_metrics']['loss']
    
    if len(losses) < 2:
        return jsonify({'error': 'Not enough data for convergence analysis'}), 400
    
    # è®¡ç®—æ”¶æ•›æŒ‡æ ‡
    recent_losses = losses[-10:] if len(losses) >= 10 else losses
    loss_change = abs(recent_losses[-1] - recent_losses[0])
    loss_std = sum((x - sum(recent_losses)/len(recent_losses))**2 for x in recent_losses) / len(recent_losses)
    loss_std = loss_std ** 0.5
    
    is_converged = loss_change < 0.001 and loss_std < 0.001
    
    return jsonify({
        'is_converged': is_converged,
        'loss_change': loss_change,
        'loss_std': loss_std,
        'convergence_rate': (losses[0] - losses[-1]) / len(losses) if len(losses) > 0 else 0,
        'total_rounds': len(losses)
    })


def add_log(message, level='info'):
    """æ·»åŠ æ—¥å¿—"""
    log_entry = {
        'timestamp': datetime.now().isoformat(),
        'level': level,
        'message': message
    }
    training_state['logs'].append(log_entry)
    
    # é™åˆ¶æ—¥å¿—æ•°é‡
    if len(training_state['logs']) > 1000:
        training_state['logs'] = training_state['logs'][-1000:]
    
    # é€šè¿‡WebSocketå¹¿æ’­æ—¥å¿—
    socketio.emit('new_log', log_entry)


# æ³¨æ„ï¼šMonitoredExperiment ç±»å·²ä¸å†ä½¿ç”¨ï¼ˆå› ä¸ºä¸å†ä¾èµ– Fed-BioMed Experimentï¼‰
# ä¿ç•™æ­¤ç±»ä»…ç”¨äºå‘åå…¼å®¹ï¼Œå®é™…è®­ç»ƒé€šè¿‡ federated_simulation_trainer.run_federated_training è¿›è¡Œ


def run_visualization_server(host='0.0.0.0', port=5002, debug=False):
    """è¿è¡Œå¯è§†åŒ–æœåŠ¡å™¨"""
    print("=" * 60)
    print(f"Starting Federated Learning Visualization Server")
    print(f"Template directory: {template_dir}")
    print(f"Static directory: {static_dir}")
    print(f"Server URL: http://{host}:{port}")
    print("=" * 60)
    print("Open your browser and navigate to the URL above to view the dashboard")
    print("")
    print("ğŸ“‹ ä½¿ç”¨è¯´æ˜:")
    print("   1. åœ¨ç½‘é¡µä¸Šæ¿€æ´»èŠ‚ç‚¹ï¼ˆnode_1, node_2, node_3ï¼‰")
    print("   2. èŠ‚ç‚¹æ¿€æ´»åä¼šå‡†å¤‡å¥½å‚ä¸è”é‚¦è®­ç»ƒ")
    print("   3. ç‚¹å‡»'å¼€å§‹è®­ç»ƒ'æŒ‰é’®å¯åŠ¨è”é‚¦è®­ç»ƒ")
    print("   4. è®­ç»ƒå°†åœ¨å•è¿›ç¨‹å†…æ¨¡æ‹Ÿå¤šå®¢æˆ·ç«¯è”é‚¦å­¦ä¹ ")
    print("=" * 60)
    try:
        socketio.run(app, host=host, port=port, debug=debug, allow_unsafe_werkzeug=True)
    except Exception as e:
        print(f"Error starting server: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    # è¿è¡ŒæœåŠ¡å™¨
    run_visualization_server(port=5002, debug=True)
